{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677115d5",
   "metadata": {},
   "source": [
    "### Explanation of Entropy in Classification\n",
    "(a) High entropy means the partition are pure. It should be false.\n",
    "(b) High entropy means the partitions are impure. It should be true.\n",
    "The answer is based on the understanding that by high entropy it refers to the absolute value of index --negative entropy. High negative entropy(absolute value) means the the feature gives results that close to randomness (probability close to 1 for binomial case). By gini impurity, it is also the same case. If the gini impurity is closer to 0.5, then it is more uncertain and random. The partition based on the tree is hence less impure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7fc68",
   "metadata": {},
   "source": [
    "###  Feature Selection Using the Funnelling Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda52189",
   "metadata": {},
   "source": [
    "The filter methods include statistical assessment of the features outside of the model. Samples include correlation assessment, Chi-square, ANOVA, variance threshold etc. Given features are in continuous values format, not categorical, the initial adopted filter methods are variance threshold, to remove static features, mutual information classification, to capture the features with information gain (including nonlinear), and correlation filtering among features, to remove duplicates. The selected features based on this are 'high', 'value'. Only 2 features out of 30 retained in this method. The model result offered an accuracy ratio at 60% over 49% regarding trained vs predicted The below are the corresponding confusion matrix and the ROC curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92b0da",
   "metadata": {},
   "source": [
    "<div style =\"display: flex;flex-wrap:wrap;\">\n",
    "    <img src=\"figures/confusion/filter0.png\" style=\"width:50%;\">\n",
    "    <img src=\"figures/roc/filter0.png\"style=\"width:50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecf654",
   "metadata": {},
   "source": [
    " But the mutual information classification takes away too many features that I subjectively think valuable, so I decided to remove this method. I also noted that pearson correlation more focuses on linear correlation, and by this method we definitely risk lossing valuable nonlinear features. The selected features are ['open', 'high', 'SMA_5', 'EMA5', 'EMA10', 'SMA_30'], based on variance threshold and mutual information classification method only. However, the performance is not improved based on the confusion matrix, ROC curve (intuitively). The accuracy rate even deteriorated in terms of prediction/actual performance. The rate is at 60% vs 46% train vs predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad5201",
   "metadata": {},
   "source": [
    "<div style =\"display: flex;flex-wrap:wrap;\">\n",
    "    <img src=\"figures/confusion/filter.png\" style=\"width:50%;\">\n",
    "    <img src=\"figures/roc/filter.png\"style=\"width:50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83068e9",
   "metadata": {},
   "source": [
    "I chose forward method for wrapping. It is based on the raw features selected rather than on the basis of the filter method. This is because filtering approach aggressively narrowed down the features. The recommended 15 features include ['open',\n",
    " 'high',\n",
    " 'low',\n",
    " 'close',\n",
    " 'volume',\n",
    " 'value',\n",
    " 'SMA_5',\n",
    " 'EMA5',\n",
    " 'SMA_10',\n",
    " 'SMA_45',\n",
    " 'atr',\n",
    " 'macd_line',\n",
    " 'bb_mid',\n",
    " 'bb_upper',\n",
    " 'rsi']. The base model output gives the accuracy at 64% amd 50% for training and test correspondingly. The confusion matrix also showed that the prediction has a unbalanced tendency to be 1 rather 0, although the y split is quite balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb9f038",
   "metadata": {},
   "source": [
    "<div style =\"display: flex;flex-wrap:wrap;\">\n",
    "    <img src=\"figures/confusion/wrapper.png\" style=\"width:50%;\">\n",
    "    <img src=\"figures/roc/wrapper.png\"style=\"width:50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf68176",
   "metadata": {},
   "source": [
    "I furthered my analysis of the features selection by using extreme gradient boosting  embedded approach. I cut the top 10 importance (by default, information gains) features. The selected ones are ['SMA_45',\n",
    " 'SMA_35',\n",
    " 'EMA45',\n",
    " 'EMA10',\n",
    " 'EMA5',\n",
    " 'low',\n",
    " 'bb_upper',\n",
    " 'EMA15',\n",
    " 'SMA_20',\n",
    " 'SMA_30']\n",
    ". The accuracy rate is at 61% vs 47%. the overfitting symtom is slightly alleviated comparing to the result from forward wrapper approach. However, the false positive prediction is still or even more severe as per the confusion matrix indicated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77e03e",
   "metadata": {},
   "source": [
    "<div style =\"display: flex;flex-wrap:wrap;\">\n",
    "    <img src=\"figures/confusion/embedded.png\" style=\"width:30%;\">\n",
    "    <img src=\"figures/roc/embedded.png\"style=\"width:30%;\">\n",
    "    <img src=\"figures/embedded/pie_chart.png\"style=\"width:30%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c8c35",
   "metadata": {},
   "source": [
    "I also tried to first narrow the selection by wrapper and then use the 15 features selected in wrapper approach and observe what is the information contribution gain for each in embedded approach. The acuracy is the same as the top n parameters is set at 15. The rest of the situation is as per praphs below. The features list: ['close',\n",
    " 'bb_upper',\n",
    " 'SMA_45',\n",
    " 'rsi',\n",
    " 'bb_mid',\n",
    " 'high',\n",
    " 'EMA5',\n",
    " 'macd_line',\n",
    " 'volume',\n",
    " 'SMA_5',\n",
    " 'open',\n",
    " 'atr',\n",
    " 'low',\n",
    " 'SMA_10',\n",
    " 'value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2277762",
   "metadata": {},
   "source": [
    "<div style =\"display: flex;flex-wrap:wrap;\">\n",
    "    <img src=\"figures/confusion/wrapper_embedded_mix.png\" style=\"width:50%;\">\n",
    "    <img src=\"figures/roc/wrapper_embedded_mix.png\"style=\"width:50%;\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf8636",
   "metadata": {},
   "source": [
    "### Model Building, Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f1e076",
   "metadata": {},
   "source": [
    "This section is also built on the basis of previous code script. The definition of postive move is defined as per previous section: should be at least 0.5% increase. This is to consider the transaction cost. On top of previous effort. This section will starts with the hyperparameter tunning on the basis of the 15 features seleted above. The parameters grid is set as per the below in script, and it is with rolling k folder cross validation approach given time series.(split by 5 and gap at 1). The hyperparameters tunning is achieved by deploying the RandomizedSearchCV function, using logistic function as the loss function and logloss function to score (given that the target is transfromed into binomial variable). The n_iter is set at 100 to allow for random 100 combinations of the hyperaprameters randomly. The hyperparameters grid is {\n",
    "            'max_depth':[3,4],\n",
    "            'learning_rate':[0.01,0.05,0.2],\n",
    "            'n_estimators':[50,80,100],\n",
    "            'subsample': [0.6,0.8,1],\n",
    "            'colsample_bytree': [0.6,0.8,1],\n",
    "            'gamma': [0.04,0.05],\n",
    "            'min_child_weight': [0.8,1,1.2]\n",
    "        }.  The max depth is kept low to make sure that the model is not overcomplicated. The learning rate takes both number smaller than 0.1 and larger than 0.1 so that both slow learning and fast learning is considered. The subsample size and colsample_bytree are set with both partial or full dataset (training). The gamma is set small as it is observed that features are with very close importance in terms of gain. Min child weight is both with more than and smaller than 1 to allow for various split threshold. The result is 67% vs 52% in terms of train vs test accuracy rate. The optimized parameters are :{'subsample': 0.8,\n",
    " 'min_child_weight': 1,\n",
    " 'max_depth': 3,\n",
    " 'gamma': 0.05,\n",
    " 'colsample_bytree': 0.8,\n",
    " 'eta': 0.05,\n",
    " 'objective': 'binary:logistic',\n",
    " 'eval_metric': 'logloss'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee579e4",
   "metadata": {},
   "source": [
    "<div style =\"display: flex;flex-wrap:wrap;\">\n",
    "    <img src=\"figures/confusion/optimized_wrapper_embedded_mix.png\" style=\"width:50%;\">\n",
    "    <img src=\"figures/roc/optimized_wrapper_embedded_mix.png\"style=\"width:50%;\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68c578",
   "metadata": {},
   "source": [
    "As a conclusion, with hyperparameters tunning the result is slightly better but still falls quite low in terms of predictabiity performance on tested dataset. It is 2% more than a random guess. The potential reason can be of not so relevant features selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a9a2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     RandomizedSearchCV,\n",
    "                                     TimeSeriesSplit\n",
    "                                    )\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance, to_graphviz\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                            auc,\n",
    "                            roc_curve,\n",
    "                            RocCurveDisplay,\n",
    "                            ConfusionMatrixDisplay,\n",
    "                            confusion_matrix\n",
    "                            )\n",
    "from sklearn.metrics import(classification_report,\n",
    "                           confusion_matrix)\n",
    "import ccxt\n",
    "import os\n",
    "from fear_and_greed import FearAndGreedIndex\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "class eth_predict():\n",
    "    def __init__(self,symbol,timeframe,selecttion_run_name='wrapper_embedded_mix'):\n",
    "        self.symbol = symbol\n",
    "        self.timeframe = timeframe\n",
    "        self.selecttion_run_name =selecttion_run_name\n",
    "        self.exchange =ccxt.binance()\n",
    "        self.fng =FearAndGreedIndex()\n",
    "        self.sentiment_cache =None\n",
    "        self.xgb_cache =None\n",
    "        self.features_cache =None\n",
    "        self.features_wrapper_cache =None\n",
    "        self.features_embedded_cache =None\n",
    "        \n",
    "    def obtain_df(self):\n",
    "        \n",
    "        exchange =self.exchange\n",
    "        symbol = self.symbol\n",
    "        timeframe = self.timeframe\n",
    "        since =exchange.parse8601('2017-01-01T00:00:00Z')\n",
    "        limit = 1000\n",
    "        df =[]\n",
    "        while True:\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol,\n",
    "                                         timeframe=timeframe,\n",
    "                                        since = since,\n",
    "                                        limit = 1000)\n",
    "            if not ohlcv:\n",
    "                break\n",
    "            df +=ohlcv\n",
    "            since = ohlcv[-1][0]+1\n",
    "        \n",
    "        df = pd.DataFrame(df,columns = ['timestamp','open','high',\n",
    "                                             'low','close','volume'])\n",
    "        df['timestamp']=pd.to_numeric(df['timestamp'],errors='coerce')\n",
    "        df['date']=pd.to_datetime(df['timestamp'],unit='ms').dt.tz_localize(None)\n",
    "        \n",
    "        df =df[['date','open','high','low','close','volume']]\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def append_sentiment(self):\n",
    "        if self.sentiment_cache is not None: \n",
    "            return self.sentiment_cache \n",
    "        fng =self.fng\n",
    "        fng_data = fng.get_historical_data(datetime.now()-timedelta(days=365*8))\n",
    "        fng_df = [(item['timestamp'],item['value']) for item in fng_data]\n",
    "        fng_df =pd.DataFrame(fng_df, columns =['timestamp','value'])\n",
    "        fng_df['timestamp']=pd.to_numeric(fng_df['timestamp'],errors='coerce')\n",
    "        fng_df['date'] =pd.to_datetime(fng_df['timestamp'],unit='s').dt.tz_localize(None)\n",
    "        fng_df['value']=pd.to_numeric(fng_df['value'],errors='coerce')\n",
    "        fng_df=fng_df[['date','value']] \n",
    "        fng_df=fng_df.set_index('date')\n",
    "\n",
    "        \n",
    "        df = self.obtain_df()\n",
    "        df = df.set_index('date')\n",
    "\n",
    "        df=df.merge(fng_df,right_index=True,left_index=True, how='left')\n",
    "        df=df.dropna()\n",
    "        self.sentiment_cache =df\n",
    "        return df\n",
    "    \n",
    "    def atr(self,period=14):\n",
    "        df =self.append_sentiment()\n",
    "        high=df['high']\n",
    "        low=df['low']\n",
    "        close=df['close']\n",
    "        tr =pd.concat([high-low,(high-close.shift()).abs(),\n",
    "                       (low-close.shift()).abs()], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=period).mean()\n",
    "        return pd.DataFrame({'atr':atr})\n",
    "    \n",
    "    def bbands(self,window=20,stds=2):\n",
    "        df =self.append_sentiment()\n",
    "        sma = df['close'].rolling(window).mean()\n",
    "        std = df['close'].rolling(window).std()\n",
    "        upper =sma +stds*std\n",
    "        lower =sma-stds*std\n",
    "        \n",
    "        return pd.DataFrame({'bb_lower':lower,'bb_mid':sma,'bb_upper':upper})\n",
    "    \n",
    "    def macd(self,fast=12,slow=26, signal=9):\n",
    "        df =self.append_sentiment()\n",
    "        ema_fast = df['close'].ewm(span=fast,adjust=False).mean()\n",
    "        ema_low = df['close'].ewm(span=slow,adjust=False).mean()\n",
    "        macd_line =ema_fast-ema_low\n",
    "        signal_line =macd_line.ewm(span=signal).mean()\n",
    "        histogram =macd_line -signal_line\n",
    "        \n",
    "        return  pd.DataFrame({'macd_line':macd_line,\n",
    "                              'signal_line':signal_line, \n",
    "                             'histogram':histogram})\n",
    "        \n",
    "    def rsi(self,period=14):\n",
    "        df =self.append_sentiment()\n",
    "        delta =df['close'].diff()\n",
    "        \n",
    "        gain =delta.where(delta>0,0.0)\n",
    "        loss =-delta.where(delta<0,0.0)\n",
    "        \n",
    "        avg_gain =gain.rolling(window=period).mean()\n",
    "        avg_loss =loss.rolling(window=period).mean()\n",
    "        \n",
    "        rs =avg_gain/avg_loss\n",
    "        rsi =100-100/(rs+1)\n",
    "        return pd.DataFrame({'rsi':rsi})\n",
    "    \n",
    "    def features_list(self):\n",
    "        df= self.append_sentiment()\n",
    "        for i in range(5,50,5):\n",
    "            df['SMA_'+str(i)]=df['close'].rolling(window=i).mean()\n",
    "            df['EMA'+str(i)]=df['close'].ewm(span=i,adjust=False).mean()\n",
    "        df = pd.concat([df,self.atr()],axis=1)\n",
    "        df = pd.concat([df,self.macd()[['macd_line']]],axis=1)\n",
    "        df = pd.concat([df,self.bbands()],axis=1)\n",
    "        df =pd.concat([df,self.rsi()],axis=1)\n",
    "        \n",
    "        df.dropna(inplace= True)\n",
    "        \n",
    "        x=df\n",
    "        #x= df.drop(['open', 'high', 'low','close'],axis=1)\n",
    "        \n",
    "        return x, df\n",
    "        \n",
    "        \n",
    "    def target_var(self):\n",
    "        df =self.features_list()[1]\n",
    "        df['target']=np.where(df['close'].shift(-1)>1.005*df['close'],1,0)\n",
    "        y =df['target']\n",
    "        return y\n",
    "    def features_filter(self, x_trained, x_test, y,variance_thresh=0.01,\n",
    "                       mi_thresh=0.005,\n",
    "                       corr_thresh=0.95):\n",
    "        if self.features_cache is not None:\n",
    "            return self.features_cache\n",
    "        \n",
    "        \n",
    "        vt =VarianceThreshold(threshold =variance_thresh)\n",
    "        x_var =pd.DataFrame(vt.fit_transform(x_trained),\n",
    "                            columns =x_trained.columns[vt.get_support()])\n",
    "        x_trained= x_var.copy()\n",
    "        mi_scores =pd.Series(mutual_info_classif(x_trained,y), index= x_trained.columns)\n",
    "        mi_selected =mi_scores[mi_scores >mi_thresh].index\n",
    "        x_mi = x_trained[mi_selected]\n",
    "        x_trained = x_mi.copy()\n",
    "        \n",
    "#         corr_matrix = x_trained.corr().abs()\n",
    "#         upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), \n",
    "#                                           k =1).astype(bool))\n",
    "#         col_drop =[col for col in x_trained.columns if any(upper[col]>corr_thresh)]\n",
    "#         x_corr =x_trained.drop(columns =col_drop)\n",
    "#         x_trained = x_corr.copy()\n",
    "        \n",
    "        selected_features =x_trained.columns.to_list()\n",
    "        x_test =x_test[selected_features]\n",
    "        x_train_scaled, x_test_scaled =self.scalling(x_trained,x_test)\n",
    "        \n",
    "        self.features_cache = x_train_scaled, x_test_scaled, selected_features\n",
    "        return x_train_scaled, x_test_scaled, selected_features\n",
    "    \n",
    "    def features_wrapper(self,x_train_scaled,x_test_scaled,y):\n",
    "        if self.features_wrapper_cache is not None:\n",
    "            return self.features_wrapper_cache\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "        \n",
    "        selector = SequentialFeatureSelector(\n",
    "            model, direction='forward', scoring='roc_auc', n_jobs=-1, cv=5\n",
    "        )\n",
    "        selector.fit(x_train_scaled, y)\n",
    "        x_trained, x_test = self.split_scalling()[0:2]\n",
    "        x_train_scaled = x_train_scaled[:,selector.get_support()]\n",
    "        x_test_scaled =x_test_scaled[:,selector.get_support()]\n",
    "        features_selected =x_trained.loc[:,selector.get_support()].columns.to_list()\n",
    "\n",
    "        self.features_wrapper_cache =x_train_scaled, x_test_scaled,features_selected\n",
    "        return x_train_scaled, x_test_scaled,features_selected\n",
    "    \n",
    "    def features_embedded(self,x_train,x_test,y,top_n=10):\n",
    "        path_name ='figures/embedded'\n",
    "            \n",
    "        if self.features_embedded_cache is not None:\n",
    "            return self.features_embedded_cache\n",
    "        \n",
    "        \n",
    "        model =XGBClassifier( use_label_encoder = False, eval_metric='logloss')\n",
    "        model.fit(x_train,y)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': x_train.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values(by='importance', ascending =False)\n",
    "        \n",
    "        selected_features = importance_df.head(top_n)['feature'].to_list()\n",
    "        x_train =x_train[selected_features].copy()\n",
    "        x_test =x_test[selected_features].copy()\n",
    "        plt.barh(importance_df['feature'][:top_n][::-1],\n",
    "                importance_df['importance'][:top_n][::-1])\n",
    "        plt.title('XGBoost Embedded Feature Importance(Top 10)')\n",
    "        plt.xlabel('feature importance')\n",
    "        os.makedirs(path_name,exist_ok =True)\n",
    "        plt.savefig(os.path.join(path_name,'pie_chart.png'))\n",
    "        \n",
    "        x_train_scaled, x_test_scaled= self.scalling(x_train, x_test)\n",
    "       \n",
    "        self.features_embedded_cache =x_train_scaled,x_test_scaled,selected_features\n",
    "        \n",
    "        return x_train_scaled,x_test_scaled,selected_features\n",
    "\n",
    "    def scalling(self,x_train,x_test):\n",
    "        \n",
    "        scaler =StandardScaler()\n",
    "        x_train_scaled =scaler.fit_transform(x_train)\n",
    "        x_test_scaled =scaler.transform(x_test)\n",
    "        \n",
    "        return x_train_scaled, x_test_scaled\n",
    "    def split_scalling(self):\n",
    "    \n",
    "        x= self.features_list()[0]\n",
    "        y =self.target_var() \n",
    "        x_train, x_test, y_train, y_test  =train_test_split(x,y, test_size=0.2, shuffle=False)\n",
    "\n",
    "        scaler=StandardScaler()\n",
    "        x_train_scaled =scaler.fit_transform(x_train)\n",
    "        x_test_scaled =scaler.transform(x_test)\n",
    "        features_selected =x_train.columns\n",
    "       \n",
    "        return x_train, x_test, y_train, y_test,x_train_scaled, x_test_scaled, features_selected\n",
    "    \n",
    "    def cwts(self):\n",
    "      \n",
    "        x_test,y_train, y_test =self.split_scalling()[1:4]\n",
    "        x_train_scaled =self.selection_run()[0]\n",
    "        x_test_scaled =self.selection_run()[1]\n",
    "        features_selected =self.selection_run()[2]\n",
    "\n",
    "                                     \n",
    "        y0, y1 =np.bincount(y_train)\n",
    "        w0=(1/y0)*(len(y_train))/2\n",
    "        w1=(1/y1)*(len(y_train))/2\n",
    "\n",
    "        train_weights =[w0 if item==0 else w1 for item in y_train]\n",
    "        dtrain =xgb.DMatrix(x_train_scaled, label =y_train, \n",
    "                            nthread=4, weight =train_weights,\n",
    "                           feature_names =features_selected)\n",
    "\n",
    "        test_weights=[w0 if item==0 else w1 for item in y_test]\n",
    "        dtest =xgb.DMatrix(x_test_scaled, label =y_test, \n",
    "                           nthread=4, weight =test_weights,\n",
    "                          feature_names =features_selected)\n",
    "        return dtrain, dtest\n",
    "    def xgb_model(self):\n",
    "        if self.xgb_cache is not None:\n",
    "            return self.xgb_cache\n",
    "        params={\n",
    "            'objective':'binary:logistic',\n",
    "            'eval_metric': 'logloss',\n",
    "            'max_depth': 3,\n",
    "            'eta': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8\n",
    "        }\n",
    "            \n",
    "        dtrain, dtest = self.cwts()\n",
    "        base_model =xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round =100,\n",
    "            evals=[(dtrain,'train'),(dtest,'eval')],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=0\n",
    "            )\n",
    "        y_prob =base_model.predict(dtest)\n",
    "        y_pred = np.round(y_prob)\n",
    "        y_pred_train =np.round(base_model.predict(dtrain))\n",
    "        self.xgb_cache =y_pred, y_pred_train\n",
    "        \n",
    "        return y_pred, y_pred_train\n",
    "    \n",
    "    def selection_run(self):\n",
    "        if self.selecttion_run_name =='filter':\n",
    "            x_train,x_test,y= self.split_scalling()[0:3] \n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_filter(x_train,x_test,y)\n",
    "            \n",
    "        elif self.selecttion_run_name =='wrapper':\n",
    "            x_train_scaled,x_test_scaled=self.split_scalling()[4:6]\n",
    "            y=self.split_scalling()[2]\n",
    "            \n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_wrapper(x_train_scaled,x_test_scaled,y)\n",
    "            \n",
    "        elif self.selecttion_run_name =='embedded':\n",
    "            x_train,x_test,y= self.split_scalling()[0:3]\n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_embedded(x_train,x_test,y)\n",
    "        else:\n",
    "            x_train_scaled,x_test_scaled=self.split_scalling()[4:6]\n",
    "            y=self.split_scalling()[2] \n",
    "            features_selected =self.features_wrapper(x_train_scaled,x_test_scaled,y)[2]\n",
    "            x_train,x_test,y= self.split_scalling()[0:3]\n",
    "            x_train =x_train.loc[:,features_selected]\n",
    "            x_test=x_test.loc[:,features_selected]\n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_embedded(x_train,x_test,y,top_n=15)\n",
    "            \n",
    "        return x_train_scaled,x_test_scaled, features_selected\n",
    "    \n",
    "    def draw_confusion(self):\n",
    "        selecttion_run_name =self.selecttion_run_name\n",
    "        path_name ='figures/confusion'\n",
    "        if not os.path.exists(path_name):\n",
    "            os.makedirs(path_name)\n",
    "        \n",
    "        y_pred =self.xgb_model()[0]\n",
    "        y_test =self.split_scalling()[3]\n",
    "        cm =confusion_matrix(y_test,y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Down (0)', 'Up (1)'])\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(f'{selecttion_run_name}_Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path_name,selecttion_run_name+'.png'),\n",
    "                    dpi=100, bbox_inches ='tight')\n",
    "        return plt.show()\n",
    "    \n",
    "    def draw_roc(self):\n",
    "        selecttion_run_name =self.selecttion_run_name\n",
    "        path_name ='figures/roc/'\n",
    "        if not os.path.exists(path_name):\n",
    "            os.makedirs(path_name)\n",
    "        y_pred= self.xgb_model()[0]\n",
    "        y_test=self.split_scalling()[3]\n",
    "        fpr, tpr, _ =roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        disp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "        disp.plot()\n",
    "\n",
    "        plt.title(f'{selecttion_run_name}_ROC Curve')\n",
    "        plt.plot([0, 1], [0, 1], color='orange', lw=2, linestyle='--')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path_name,selecttion_run_name+'.png'),\n",
    "                    dpi=300,bbox_inches='tight')\n",
    "        return plt.show()\n",
    "        \n",
    "    def accuracy_check(self):\n",
    "        y_train, y_test =self.split_scalling()[2:4]\n",
    "        dtrain =self.cwts()[0]\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_preds = self.xgb_model()[1]\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_preds = self.xgb_model()[0]\n",
    "        test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "        return [print(f\"Training Accuracy: {train_accuracy:.4f}\"),\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\"),\n",
    "        print(f\"Difference (Training - Test): {train_accuracy - test_accuracy:.4f}\")]\n",
    "    \n",
    "\n",
    "    \n",
    "handle = eth_predict('ETH/USDT','1d','filter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "928c553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import (train_test_split, \n",
    "                                     RandomizedSearchCV,\n",
    "                                     TimeSeriesSplit\n",
    "                                    )\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance, to_graphviz\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                            auc,\n",
    "                            roc_curve,\n",
    "                            RocCurveDisplay,\n",
    "                            ConfusionMatrixDisplay,\n",
    "                            confusion_matrix\n",
    "                            )\n",
    "from sklearn.metrics import(classification_report,\n",
    "                           confusion_matrix)\n",
    "import ccxt\n",
    "import os\n",
    "from fear_and_greed import FearAndGreedIndex\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "class eth_predict_optimized():\n",
    "    def __init__(self,symbol,timeframe,selecttion_run_name='optimized_wrapper_embedded_mix'):\n",
    "        self.symbol = symbol\n",
    "        self.timeframe = timeframe\n",
    "        self.selecttion_run_name =selecttion_run_name\n",
    "        self.exchange =ccxt.binance()\n",
    "        self.fng =FearAndGreedIndex()\n",
    "        self.sentiment_cache =None\n",
    "        self.xgb_cache =None\n",
    "        self.features_cache =None\n",
    "        self.features_wrapper_cache =None\n",
    "        self.features_embedded_cache =None\n",
    "        self.hyperparams_optimized_cache =None\n",
    "        self.params_grid ={\n",
    "            'max_depth':[3,4],\n",
    "            'learning_rate':[0.01,0.05,0.2],\n",
    "            'n_estimators':[50,80,100],\n",
    "            'subsample': [0.6,0.8,1],\n",
    "            'colsample_bytree': [0.6,0.8,1],\n",
    "            'gamma': [0.04,0.05],\n",
    "            'min_child_weight': [0.8,1,1.2]\n",
    "        }\n",
    "        self.optimized_model_cache= None\n",
    "        \n",
    "    def obtain_df(self):\n",
    "        \n",
    "        exchange =self.exchange\n",
    "        symbol = self.symbol\n",
    "        timeframe = self.timeframe\n",
    "        since =exchange.parse8601('2017-01-01T00:00:00Z')\n",
    "        limit = 1000\n",
    "        df =[]\n",
    "        while True:\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol,\n",
    "                                         timeframe=timeframe,\n",
    "                                        since = since,\n",
    "                                        limit = 1000)\n",
    "            if not ohlcv:\n",
    "                break\n",
    "            df +=ohlcv\n",
    "            since = ohlcv[-1][0]+1\n",
    "        \n",
    "        df = pd.DataFrame(df,columns = ['timestamp','open','high',\n",
    "                                             'low','close','volume'])\n",
    "        df['timestamp']=pd.to_numeric(df['timestamp'],errors='coerce')\n",
    "        df['date']=pd.to_datetime(df['timestamp'],unit='ms').dt.tz_localize(None)\n",
    "        \n",
    "        df =df[['date','open','high','low','close','volume']]\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def append_sentiment(self):\n",
    "        if self.sentiment_cache is not None: \n",
    "            return self.sentiment_cache \n",
    "        fng =self.fng\n",
    "        fng_data = fng.get_historical_data(datetime.now()-timedelta(days=365*8))\n",
    "        fng_df = [(item['timestamp'],item['value']) for item in fng_data]\n",
    "        fng_df =pd.DataFrame(fng_df, columns =['timestamp','value'])\n",
    "        fng_df['timestamp']=pd.to_numeric(fng_df['timestamp'],errors='coerce')\n",
    "        fng_df['date'] =pd.to_datetime(fng_df['timestamp'],unit='s').dt.tz_localize(None)\n",
    "        fng_df['value']=pd.to_numeric(fng_df['value'],errors='coerce')\n",
    "        fng_df=fng_df[['date','value']] \n",
    "        fng_df=fng_df.set_index('date')\n",
    "\n",
    "        \n",
    "        df = self.obtain_df()\n",
    "        df = df.set_index('date')\n",
    "\n",
    "        df=df.merge(fng_df,right_index=True,left_index=True, how='left')\n",
    "        df=df.dropna()\n",
    "        self.sentiment_cache =df\n",
    "        return df\n",
    "    \n",
    "    def atr(self,period=14):\n",
    "        df =self.append_sentiment()\n",
    "        high=df['high']\n",
    "        low=df['low']\n",
    "        close=df['close']\n",
    "        tr =pd.concat([high-low,(high-close.shift()).abs(),\n",
    "                       (low-close.shift()).abs()], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=period).mean()\n",
    "        return pd.DataFrame({'atr':atr})\n",
    "    \n",
    "    def bbands(self,window=20,stds=2):\n",
    "        df =self.append_sentiment()\n",
    "        sma = df['close'].rolling(window).mean()\n",
    "        std = df['close'].rolling(window).std()\n",
    "        upper =sma +stds*std\n",
    "        lower =sma-stds*std\n",
    "        \n",
    "        return pd.DataFrame({'bb_lower':lower,'bb_mid':sma,'bb_upper':upper})\n",
    "    \n",
    "    def macd(self,fast=12,slow=26, signal=9):\n",
    "        df =self.append_sentiment()\n",
    "        ema_fast = df['close'].ewm(span=fast,adjust=False).mean()\n",
    "        ema_low = df['close'].ewm(span=slow,adjust=False).mean()\n",
    "        macd_line =ema_fast-ema_low\n",
    "        signal_line =macd_line.ewm(span=signal).mean()\n",
    "        histogram =macd_line -signal_line\n",
    "        \n",
    "        return  pd.DataFrame({'macd_line':macd_line,\n",
    "                              'signal_line':signal_line, \n",
    "                             'histogram':histogram})\n",
    "        \n",
    "    def rsi(self,period=14):\n",
    "        df =self.append_sentiment()\n",
    "        delta =df['close'].diff()\n",
    "        \n",
    "        gain =delta.where(delta>0,0.0)\n",
    "        loss =-delta.where(delta<0,0.0)\n",
    "        \n",
    "        avg_gain =gain.rolling(window=period).mean()\n",
    "        avg_loss =loss.rolling(window=period).mean()\n",
    "        \n",
    "        rs =avg_gain/avg_loss\n",
    "        rsi =100-100/(rs+1)\n",
    "        return pd.DataFrame({'rsi':rsi})\n",
    "    \n",
    "    def features_list(self):\n",
    "        df= self.append_sentiment()\n",
    "        for i in range(5,50,5):\n",
    "            df['SMA_'+str(i)]=df['close'].rolling(window=i).mean()\n",
    "            df['EMA'+str(i)]=df['close'].ewm(span=i,adjust=False).mean()\n",
    "        df = pd.concat([df,self.atr()],axis=1)\n",
    "        df = pd.concat([df,self.macd()[['macd_line']]],axis=1)\n",
    "        df = pd.concat([df,self.bbands()],axis=1)\n",
    "        df =pd.concat([df,self.rsi()],axis=1)\n",
    "        \n",
    "        df.dropna(inplace= True)\n",
    "        \n",
    "        x=df\n",
    "        x= df.drop(['open', 'high', 'low','close'],axis=1)\n",
    "        \n",
    "        return x, df\n",
    "        \n",
    "        \n",
    "    def target_var(self):\n",
    "        df =self.features_list()[1]\n",
    "        df['target']=np.where(df['close'].shift(-1)>1.005*df['close'],1,0)\n",
    "        y =df['target']\n",
    "        return y\n",
    "    def features_filter(self, x_trained, x_test, y,variance_thresh=0.01,\n",
    "                       mi_thresh=0.005,\n",
    "                       corr_thresh=0.95):\n",
    "        if self.features_cache is not None:\n",
    "            return self.features_cache\n",
    "        \n",
    "        \n",
    "        vt =VarianceThreshold(threshold =variance_thresh)\n",
    "        x_var =pd.DataFrame(vt.fit_transform(x_trained),\n",
    "                            columns =x_trained.columns[vt.get_support()])\n",
    "        x_trained= x_var.copy()\n",
    "        mi_scores =pd.Series(mutual_info_classif(x_trained,y), index= x_trained.columns)\n",
    "        mi_selected =mi_scores[mi_scores >mi_thresh].index\n",
    "        x_mi = x_trained[mi_selected]\n",
    "        x_trained = x_mi.copy()\n",
    "        \n",
    "#         corr_matrix = x_trained.corr().abs()\n",
    "#         upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), \n",
    "#                                           k =1).astype(bool))\n",
    "#         col_drop =[col for col in x_trained.columns if any(upper[col]>corr_thresh)]\n",
    "#         x_corr =x_trained.drop(columns =col_drop)\n",
    "#         x_trained = x_corr.copy()\n",
    "        \n",
    "        selected_features =x_trained.columns.to_list()\n",
    "        x_test =x_test[selected_features]\n",
    "        x_train_scaled, x_test_scaled =self.scalling(x_trained,x_test)\n",
    "        \n",
    "        self.features_cache = x_train_scaled, x_test_scaled, selected_features\n",
    "        return x_train_scaled, x_test_scaled, selected_features\n",
    "    \n",
    "    def features_wrapper(self,x_train_scaled,x_test_scaled,y):\n",
    "        if self.features_wrapper_cache is not None:\n",
    "            return self.features_wrapper_cache\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "        \n",
    "        selector = SequentialFeatureSelector(\n",
    "            model, direction='forward', scoring='roc_auc', n_jobs=-1, cv=5\n",
    "        )\n",
    "        selector.fit(x_train_scaled, y)\n",
    "        x_trained, x_test = self.split_scalling()[0:2]\n",
    "        x_train_scaled = x_train_scaled[:,selector.get_support()]\n",
    "        x_test_scaled =x_test_scaled[:,selector.get_support()]\n",
    "        features_selected =x_trained.loc[:,selector.get_support()].columns.to_list()\n",
    "\n",
    "        self.features_wrapper_cache =x_train_scaled, x_test_scaled,features_selected\n",
    "        return x_train_scaled, x_test_scaled,features_selected\n",
    "    \n",
    "    def features_embedded(self,x_train,x_test,y,top_n=10):\n",
    "        path_name ='figures/embedded'\n",
    "            \n",
    "        if self.features_embedded_cache is not None:\n",
    "            return self.features_embedded_cache\n",
    "        \n",
    "        \n",
    "        model =XGBClassifier( use_label_encoder = False, eval_metric='logloss')\n",
    "        model.fit(x_train,y)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': x_train.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values(by='importance', ascending =False)\n",
    "        \n",
    "        selected_features = importance_df.head(top_n)['feature'].to_list()\n",
    "        x_train =x_train[selected_features].copy()\n",
    "        x_test =x_test[selected_features].copy()\n",
    "        plt.barh(importance_df['feature'][:top_n][::-1],\n",
    "                importance_df['importance'][:top_n][::-1])\n",
    "        plt.title('XGBoost Embedded Feature Importance(Top 10)')\n",
    "        plt.xlabel('feature importance')\n",
    "        os.makedirs(path_name,exist_ok =True)\n",
    "        plt.savefig(os.path.join(path_name,'pie_chart.png'))\n",
    "        \n",
    "        x_train_scaled, x_test_scaled= self.scalling(x_train, x_test)\n",
    "       \n",
    "        self.features_embedded_cache =x_train_scaled,x_test_scaled,selected_features\n",
    "        \n",
    "        return x_train_scaled,x_test_scaled,selected_features\n",
    "\n",
    "    def scalling(self,x_train,x_test):\n",
    "        \n",
    "        scaler =StandardScaler()\n",
    "        x_train_scaled =scaler.fit_transform(x_train)\n",
    "        x_test_scaled =scaler.transform(x_test)\n",
    "        \n",
    "        return x_train_scaled, x_test_scaled\n",
    "    def split_scalling(self):\n",
    "    \n",
    "        x= self.features_list()[0]\n",
    "        y =self.target_var() \n",
    "        x_train, x_test, y_train, y_test  =train_test_split(x,y, test_size=0.2, shuffle=False)\n",
    "\n",
    "        scaler=StandardScaler()\n",
    "        x_train_scaled =scaler.fit_transform(x_train)\n",
    "        x_test_scaled =scaler.transform(x_test)\n",
    "        features_selected =x_train.columns\n",
    "       \n",
    "        return x_train, x_test, y_train, y_test,x_train_scaled, x_test_scaled, features_selected\n",
    "    \n",
    "    def cwts(self):\n",
    "      \n",
    "        x_test,y_train, y_test =self.split_scalling()[1:4]\n",
    "        x_train_scaled =self.selection_run()[0]\n",
    "        x_test_scaled =self.selection_run()[1]\n",
    "        features_selected =self.selection_run()[2]\n",
    "\n",
    "                                     \n",
    "        y0, y1 =np.bincount(y_train)\n",
    "        w0=(1/y0)*(len(y_train))/2\n",
    "        w1=(1/y1)*(len(y_train))/2\n",
    "\n",
    "        train_weights =[w0 if item==0 else w1 for item in y_train]\n",
    "        dtrain =xgb.DMatrix(x_train_scaled, label =y_train, \n",
    "                            nthread=4, weight =train_weights,\n",
    "                           feature_names =features_selected)\n",
    "\n",
    "        test_weights=[w0 if item==0 else w1 for item in y_test]\n",
    "        dtest =xgb.DMatrix(x_test_scaled, label =y_test, \n",
    "                           nthread=4, weight =test_weights,\n",
    "                          feature_names =features_selected)\n",
    "        return dtrain, dtest\n",
    "    def xgb_classifier(self, Objective='binary:logistic',Eval_metric ='logloss'):\n",
    "        Xgb_classifier = XGBClassifier(\n",
    "            objective=Objective,\n",
    "            eval_metric=Eval_metric,)\n",
    "        \n",
    "        return Xgb_classifier\n",
    "    \n",
    "    def tscv(self, N_splits=5, Gap=1):\n",
    "        Tscv=TimeSeriesSplit(n_splits=N_splits, gap=Gap)\n",
    "        \n",
    "        return Tscv\n",
    "    def hyperparams_tunning(self):\n",
    "        if self.hyperparams_optimized_cache is not None:\n",
    "            return self.hyperparams_optimized_cache\n",
    "        xgb_classifer =self.xgb_classifier()\n",
    "        params_grid=self.params_grid\n",
    "        tscv=self.tscv()\n",
    "        x_train_scaled =self.selection_run()[0]\n",
    "        y_train =self.split_scalling()[2]\n",
    "        \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator =xgb_classifer,\n",
    "            param_distributions =params_grid,\n",
    "            n_iter =500,\n",
    "            scoring='roc_auc',\n",
    "            cv=tscv,\n",
    "            verbose=10,\n",
    "            n_jobs=-1)\n",
    "        \n",
    "        \n",
    "        random_search.fit(x_train_scaled,y_train)\n",
    "        \n",
    "        hyperparams_optimized =random_search.best_params_\n",
    "        \n",
    "        self.hyperparams_optimized_cache=random_search.best_params_\n",
    "        \n",
    "        return hyperparams_optimized\n",
    "    \n",
    "    def optimized_model(self):\n",
    "        if self.optimized_model_cache is not None:\n",
    "            return self.optimized_model_cache\n",
    "        optimized_params =self.hyperparams_tunning()\n",
    "        optimized_params['eta']=  optimized_params.pop('learning_rate')\n",
    "        optimized_params['objective']='binary:logistic'\n",
    "        optimized_params['eval_metric']='logloss'\n",
    "        dtrain,dtest =self.cwts()\n",
    "        optimized_model =xgb.train(\n",
    "            optimized_params,\n",
    "            dtrain,\n",
    "            num_boost_round =optimized_params.pop('n_estimators'),\n",
    "            evals= [(dtrain,'train'),(dtest,'eval')],\n",
    "            #early_stopping_rounds=20,\n",
    "            verbose_eval =5\n",
    "        )\n",
    "        y_proba =optimized_model.predict(dtest)\n",
    "        y_pred = np.round(y_proba)\n",
    "        \n",
    "        y_pred_train =np.round(optimized_model.predict(dtrain))\n",
    "        self.optimized_model_cache =y_pred, y_pred_train, optimized_model\n",
    "        return y_pred, y_pred_train, optimized_model\n",
    "    \n",
    "    def selection_run(self):\n",
    "        if self.selecttion_run_name =='filter':\n",
    "            x_train,x_test,y= self.split_scalling()[0:3] \n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_filter(x_train,x_test,y)\n",
    "            \n",
    "        elif self.selecttion_run_name =='wrapper':\n",
    "            x_train_scaled,x_test_scaled=self.split_scalling()[4:6]\n",
    "            y=self.split_scalling()[2]\n",
    "            \n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_wrapper(x_train_scaled,x_test_scaled,y)\n",
    "            \n",
    "        elif self.selecttion_run_name =='embedded':\n",
    "            x_train,x_test,y= self.split_scalling()[0:3]\n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_embedded(x_train,x_test,y)\n",
    "        else:\n",
    "            x_train_scaled,x_test_scaled=self.split_scalling()[4:6]\n",
    "            y=self.split_scalling()[2] \n",
    "            features_selected =self.features_wrapper(x_train_scaled,x_test_scaled,y)[2]\n",
    "            x_train,x_test,y= self.split_scalling()[0:3]\n",
    "            x_train =x_train.loc[:,features_selected]\n",
    "            x_test=x_test.loc[:,features_selected]\n",
    "            x_train_scaled,x_test_scaled,features_selected =self.features_embedded(x_train,x_test,y,top_n=15)\n",
    "            \n",
    "        return x_train_scaled,x_test_scaled, features_selected\n",
    "    \n",
    "    def draw_confusion(self):\n",
    "        selecttion_run_name =self.selecttion_run_name\n",
    "        path_name ='figures/confusion'\n",
    "        if not os.path.exists(path_name):\n",
    "            os.makedirs(path_name)\n",
    "        \n",
    "        y_pred =self.optimized_model()[0]\n",
    "        y_test =self.split_scalling()[3]\n",
    "        cm =confusion_matrix(y_test,y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Down (0)', 'Up (1)'])\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(f'{selecttion_run_name}_Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path_name,selecttion_run_name+'.png'),\n",
    "                    dpi=100, bbox_inches ='tight')\n",
    "        return plt.show()\n",
    "    \n",
    "    def draw_roc(self):\n",
    "        selecttion_run_name =self.selecttion_run_name\n",
    "        path_name ='figures/roc/'\n",
    "        if not os.path.exists(path_name):\n",
    "            os.makedirs(path_name)\n",
    "        y_pred= self.optimized_model()[0]\n",
    "        y_test=self.split_scalling()[3]\n",
    "        fpr, tpr, _ =roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        disp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "        disp.plot()\n",
    "\n",
    "        plt.title(f'{selecttion_run_name}_ROC Curve')\n",
    "        plt.plot([0, 1], [0, 1], color='orange', lw=2, linestyle='--')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(path_name,selecttion_run_name+'.png'),\n",
    "                    dpi=300,bbox_inches='tight')\n",
    "        return plt.show()\n",
    "        \n",
    "    def accuracy_check(self):\n",
    "        y_train, y_test =self.split_scalling()[2:4]\n",
    "        dtrain =self.cwts()[0]\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_preds = self.optimized_model()[1]\n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_preds = self.optimized_model()[0]\n",
    "        test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "        return [print(f\"Training Accuracy: {train_accuracy:.4f}\"),\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\"),\n",
    "        print(f\"Difference (Training - Test): {train_accuracy - test_accuracy:.4f}\")]\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "handle = eth_predict_optimized('ETH/USDT','1d','optimized_wrapper_embedded_mix')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
